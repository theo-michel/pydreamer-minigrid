{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%set_env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theomichel/work/pyDreamer/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(action_dim=18, actor_dist='onehot', actor_grad='reinforce', adam_eps=1e-05, adam_lr=0.0003, adam_lr_actor=0.0001, adam_lr_critic=0.0001, allow_mid_reset=True, amp=False, aux_critic=False, aux_critic_weight=1.0, batch_length=48, batch_size=32, buffer_size=10000000, buffer_size_offline=0, clip_rewards='tanh', cnn_depth=48, data_workers=4, deter_dim=1024, device='cuda:0', enable_profiler=False, entropy=0.001, env_action_repeat=4, env_id='Atari-Alien-V5', env_id_eval=None, env_no_terminal=False, env_time_limit=27000, eval_batch_size=32, eval_batches=61, eval_interval=2000, eval_samples=1, eval_save_size=1, gamma=0.99, gamma_aux=0.99, generator_prefill_policy='random', generator_prefill_steps=50000, generator_workers=1, generator_workers_eval=0, generator_workers_train=0, goals_size=0, grad_clip=200, grad_clip_ac=200, gru_layers=1, gru_type='gru', hidden_dim=1000, imag_horizon=15, image_categorical=False, image_channels=3, image_decoder='cnn', image_decoder_layers=0, image_decoder_min_prob=0, image_encoder='cnn', image_encoder_layers=0, image_key='image', image_size=64, image_weight=1.0, iwae_samples=1, keep_state=True, kl_balance=0.8, kl_weight=0.1, lambda_gae=0.95, lambda_gae_aux=0.95, layer_norm=True, limit_step_ratio=0, log_interval=100, logbatch_interval=1000, map_categorical=True, map_channels=0, map_decoder='dense', map_hidden_dim=1024, map_hidden_layers=4, map_key=None, map_size=0, model='dreamer', n_env_steps=200000000, n_steps=100000000, offline_data_dir=None, offline_eval_dir=None, offline_prefill_dir=None, offline_test_dir=None, probe_gradients=False, probe_model='none', reset_interval=200, reward_decoder_categorical=None, reward_decoder_layers=4, reward_input=False, reward_weight=1.0, save_interval=500, stoch_dim=32, stoch_discrete=32, target_interval=100, target_interval_aux=1000, terminal_decoder_layers=4, terminal_weight=1.0, test_batch_size=10, test_batches=61, test_save_size=1, vecobs_size=0, vecobs_weight=1.0, verbose=False)\n",
      "[launcher]  Started mlflow run f8e52d43da7a4e50ae53f02ce2b5cca9 (None) in local/0\u001b[0m\n",
      "torch.Size([48, 32, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from pydreamer.models.dreamer import Dreamer\n",
    "from pydreamer.tools import mlflow_load_checkpoint\n",
    "from pydreamer.tools import (configure_logging, mlflow_log_params,\n",
    "                             mlflow_init, print_once, read_yamls)\n",
    "from distutils.util import strtobool\n",
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "from torchsummary import summary\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_worker_info():\n",
    "    worker_type = None\n",
    "    worker_index = None\n",
    "\n",
    "    if 'TF_CONFIG' in os.environ:\n",
    "        # TF_CONFIG indicates Google Vertex AI run\n",
    "        tf_config = json.loads(os.environ['TF_CONFIG'])\n",
    "        print_once('TF_CONFIG is set:', tf_config)\n",
    "        if tf_config['cluster'].get('worker'):\n",
    "            # If there are workers in the cluster, then it's a distributed run\n",
    "            worker_type = {\n",
    "                'chief': 'learner',\n",
    "                'worker': 'generator',\n",
    "            }[str(tf_config['task']['type'])]\n",
    "            worker_index = int(tf_config['task']['index'])\n",
    "            print_once('Distributed run detected, current worker is:', f'{worker_type} ({worker_index})')\n",
    "\n",
    "    return worker_type, worker_index\n",
    "\n",
    "configure_logging('[launcher]')\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--configs', nargs='+', required=True)\n",
    "# args, remaining = parser.parse_known_args()\n",
    "#--configs defaults atari --env_id Atari-Alien-V5\n",
    "# Config from YAML\n",
    "args_list = ['defaults', 'atari']\n",
    "remaining = ['--env_id', 'Atari-Alien-V5']\n",
    "conf = {}\n",
    "configs = read_yamls('./config')\n",
    "for name in args_list:\n",
    "    if ',' in name:\n",
    "        for n in name.split(','):\n",
    "            conf.update(configs[n])\n",
    "    else:\n",
    "        conf.update(configs[name])\n",
    "\n",
    "# Override config from command-line\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "for key, value in conf.items():\n",
    "    type_ = type(value) if value is not None else str\n",
    "    if type_ == bool:\n",
    "        type_ = lambda x: bool(strtobool(x))\n",
    "    parser.add_argument(f'--{key}', type=type_, default=value)\n",
    "conf = parser.parse_args(remaining)\n",
    "\n",
    "print(conf)\n",
    "\n",
    "worker_type, worker_index = get_worker_info()\n",
    "is_main_worker = worker_type is None or worker_type == 'learner'\n",
    "mlrun = mlflow_init(wait_for_resume=not is_main_worker)\n",
    "artifact_uri = mlrun.info.artifact_uri\n",
    "mlflow_log_params(vars(conf))\n",
    "model = Dreamer(conf)\n",
    "optimizers=tuple()\n",
    "map_location=None#Same place that it is loaded from\n",
    "path = \"/home/theomichel/work/pyDreamer/pydreamer-minigrid/mlruns/0/c1a7c69b35fa4570915c6be36f57f2c9/artifacts/checkpoints/latest.pt\"\n",
    "checkpoint = torch.load(path, map_location=map_location)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "for i, opt in enumerate(optimizers):\n",
    "    opt.load_state_dict(checkpoint[f'optimizer_{i}_state_dict'])\n",
    "    checkpoint['epoch']\n",
    "\n",
    "\n",
    "## Evaluate the model\n",
    "from train import evaluate\n",
    "from pydreamer.data import DataSequential, MlflowEpisodeRepository\n",
    "from torch.utils.data import DataLoader\n",
    "from pydreamer.preprocessing import Preprocessor, WorkerInfoPreprocess\n",
    "\n",
    "preprocess = Preprocessor(image_categorical=conf.image_channels if conf.image_categorical else None,\n",
    "                            image_key=conf.image_key,\n",
    "                            map_categorical=conf.map_channels if conf.map_categorical else None,\n",
    "                            map_key=conf.map_key,\n",
    "                            action_dim=conf.action_dim,\n",
    "                            clip_rewards=conf.clip_rewards,\n",
    "                            amp=conf.amp and device.type == 'cuda')\n",
    "\n",
    "\n",
    "device = torch.device(conf.device)\n",
    "steps = 10\n",
    "model.to(device)#Important\n",
    "\n",
    "\n",
    "with open('states/in_state_alien_new10.pkl','rb') as f:\n",
    "    in_state = pickle.load(f)\n",
    "with open('states/obs_alien_new10.pkl','rb') as f:\n",
    "    obs = pickle.load(f)\n",
    "print(obs['action'].shape)\n",
    "for key in obs:\n",
    "    obs[key] = obs[key].to(device)\n",
    "#TODO in state are being resaved in the loop, get one clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "in_state_new = (in_state[0].to(device),in_state[1].to(device))\n",
    "\n",
    "#Forward is just used when the model is translating the world into features for the policy to take a decision\n",
    "# features, out_state = model.wm.forward(obs,in_state_new)\n",
    "#Training step does all evem the image prediction\n",
    "\n",
    "# loss, features, states, out_state, metrics, tensors = model.wm.training_step(obs,in_state_new,do_image_pred=True)\n",
    "\n",
    "def tensor_to_image(tensors,image_num=0,dream_num=0):\n",
    "    image_cpu = tensors['image_pred'].cpu().numpy()\n",
    "    image = image_cpu[image_num,dream_num,:,:,:].transpose(1,2,0)\n",
    "    image_final = ((image + 0.5) * 255.0).clip(0, 255).astype('uint8')\n",
    "    # ar = (image+0.5)* 255\n",
    "    # print(np.min(ar))\n",
    "    # print(np.max(ar))\n",
    "    # image_final = ar.astype(np.uint8)\n",
    "    img_rgb = image_final#[...,::-1]\n",
    "    del image_cpu\n",
    "    return img_rgb\n",
    "\n",
    "def save_avg(obs,in_state_new,img_shape,save_path=\"images/avg\",image_num=2,dream_num=2,l=1000):\n",
    "    # fig, axs = plt.subplots(1, l,figsize=(20,80))\n",
    "    avg = np.zeros(img_shape)\n",
    "    for i in tqdm(range(l)):\n",
    "        with torch.no_grad():#imag_horizon=15,\n",
    "            _, _, _, tensors, dream_tensors = model.training_step(obs,in_state_new,imag_horizon=15,do_image_pred=True,do_dream_tensors=True)#Changed to model.training_step\n",
    "        img_rgb = tensor_to_image(dream_tensors,image_num=image_num,dream_num=dream_num)#dream tensors or tensors ?\n",
    "        avg += img_rgb\n",
    "        del tensors\n",
    "        del dream_tensors\n",
    "        # axs[i].imshow(diff)\n",
    "    avg = avg/l\n",
    "    im = Image.fromarray((avg).astype(np.uint8))\n",
    "    im.save(f\"{save_path}-{l}.png\")\n",
    "    return avg\n",
    "\n",
    "# im = Image.fromarray(img_rgb)\n",
    "# im.save(\"predicted_dream_alien.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 32, 2048])\n",
      "torch.Size([48, 32, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "with open('features_dream2/feature_alien_new8.pkl','rb') as f:\n",
    "    features_dream = pickle.load(f)\n",
    "#test directed decoding with the world model\n",
    "print(features_dream.shape)\n",
    "with torch.no_grad():\n",
    "    out = model.wm.decoder.image.forward(features_dream)\n",
    "\n",
    "print(out.shape)\n",
    "# show image\n",
    "img = out[2,2,:,:,:].cpu().numpy().transpose(1,2,0)\n",
    "img = ((img + 0.5) * 255.0).clip(0, 255).astype('uint8')\n",
    "# img_rgb = img[...,::-1]\n",
    "im = Image.fromarray(img)\n",
    "im.save(\"dream_from_features_2.jpeg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop each latent variable to zero and see the image prediction\n",
    "# with torch.no_grad():#imag_horizon=15,\n",
    "#     losses, new_state, loss_metrics, tensors, dream_tensors = model.training_step(obs,in_state_new,imag_horizon=15,do_image_pred=True,do_dream_tensors=True)#Changed to model.training_step\n",
    "\n",
    "# #pick image and dream\n",
    "\n",
    "# origin_prediction = tensor_to_image(tensors,image_num=2,dream_num=2)\n",
    "# print(tensors.keys())\n",
    "# del tensors\n",
    "\n",
    "# print(origin_prediction.shape)\n",
    "\n",
    "\n",
    "# l = 10\n",
    "# fig, axs = plt.subplots(1, l,figsize=(20,80))\n",
    "# for i in range(l):\n",
    "#     # instate_copy = in_state_new[1].clone()\n",
    "#     # instate_copy[:,i] = 0\n",
    "#     # in_state_new =(in_state_new[0],instate_copy)\n",
    "#     with torch.no_grad():#imag_horizon=15,\n",
    "#         losses, new_state, loss_metrics, tensors, dream_tensors = model.training_step(obs,in_state_new,imag_horizon=15,do_image_pred=True,do_dream_tensors=True)#Changed to model.training_step\n",
    "#     img_rgb = tensor_to_image(tensors,image_num=2,dream_num=2)\n",
    "#     del tensors\n",
    "#     axs[i].imshow(img_rgb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:24<00:14,  2.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdel\u001b[39;00m tensors\n\u001b[1;32m     11\u001b[0m \u001b[39m#now we compare the difference when modyfing h\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m avg1 \u001b[39m=\u001b[39m save_avg(obs,in_state_new,origin_prediction\u001b[39m.\u001b[39;49mshape,save_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mimages/avg_new_unmodif\u001b[39;49m\u001b[39m\"\u001b[39;49m,image_num\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,dream_num\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,l\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# copy = (in_state_new[0].clone(),in_state_new[1].clone())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m#modify the in state and see the effect\u001b[39;00m\n\u001b[1;32m     15\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(in_state_new[\u001b[39m1\u001b[39m])\u001b[39m#set everything to zero in h\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36msave_avg\u001b[0;34m(obs, in_state_new, img_shape, save_path, image_num, dream_num, l)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(l)):\n\u001b[1;32m     29\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\u001b[39m#imag_horizon=15,\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m         _, _, _, tensors, dream_tensors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtraining_step(obs,in_state_new,imag_horizon\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,do_image_pred\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,do_dream_tensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m#Changed to model.training_step\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     img_rgb \u001b[39m=\u001b[39m tensor_to_image(dream_tensors,image_num\u001b[39m=\u001b[39mimage_num,dream_num\u001b[39m=\u001b[39mdream_num)\u001b[39m#dream tensors or tensors ?\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     avg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m img_rgb\n",
      "File \u001b[0;32m~/work/pyDreamer/pydreamer-minigrid/pydreamer/models/dreamer.py:153\u001b[0m, in \u001b[0;36mDreamer.training_step\u001b[0;34m(self, obs, in_state, iwae_samples, imag_horizon, do_open_loop, do_image_pred, do_dream_tensors)\u001b[0m\n\u001b[1;32m    134\u001b[0m I, H \u001b[39m=\u001b[39m iwae_samples, imag_horizon\n\u001b[1;32m    136\u001b[0m \u001b[39m# Intercepting the observation and in state to be reused in extract model\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m# Forward (world model)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m# generate random number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[39m# World model\u001b[39;00m\n\u001b[1;32m    152\u001b[0m loss_model, features, states, out_state, metrics, tensors \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwm\u001b[39m.\u001b[39;49mtraining_step(obs,\n\u001b[1;32m    154\u001b[0m                           in_state,\n\u001b[1;32m    155\u001b[0m                           iwae_samples\u001b[39m=\u001b[39;49miwae_samples,\n\u001b[1;32m    156\u001b[0m                           do_open_loop\u001b[39m=\u001b[39;49mdo_open_loop,\n\u001b[1;32m    157\u001b[0m                           do_image_pred\u001b[39m=\u001b[39;49mdo_image_pred)\n\u001b[1;32m    159\u001b[0m \u001b[39m# Map probe\u001b[39;00m\n\u001b[1;32m    161\u001b[0m features_probe \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mdetach() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobe_gradients \u001b[39melse\u001b[39;00m features\n",
      "File \u001b[0;32m~/work/pyDreamer/pydreamer-minigrid/pydreamer/models/dreamer.py:368\u001b[0m, in \u001b[0;36mWorldModel.training_step\u001b[0;34m(self, obs, in_state, iwae_samples, do_open_loop, do_image_pred, forward_only)\u001b[0m\n\u001b[1;32m    352\u001b[0m embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(obs)\n\u001b[1;32m    354\u001b[0m \u001b[39m# Intercepting the observation and in state to be reused in extract model\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m# Forward (world model)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m#generate random number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[39m# RSSM\u001b[39;00m\n\u001b[1;32m    367\u001b[0m prior, post, post_samples, features, states, out_state \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39;49mforward(embed,\n\u001b[1;32m    369\u001b[0m                       obs[\u001b[39m'\u001b[39;49m\u001b[39maction\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    370\u001b[0m                       obs[\u001b[39m'\u001b[39;49m\u001b[39mreset\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    371\u001b[0m                       in_state,\n\u001b[1;32m    372\u001b[0m                       iwae_samples\u001b[39m=\u001b[39;49miwae_samples,\n\u001b[1;32m    373\u001b[0m                       do_open_loop\u001b[39m=\u001b[39;49mdo_open_loop)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m forward_only:\n\u001b[1;32m    376\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m), features, states, out_state, {}, {}\n",
      "File \u001b[0;32m~/work/pyDreamer/pydreamer-minigrid/pydreamer/models/rssm.py:51\u001b[0m, in \u001b[0;36mRSSMCore.forward\u001b[0;34m(self, embed, action, reset, in_state, iwae_samples, do_open_loop)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m do_open_loop:\n\u001b[0;32m---> 51\u001b[0m         post, (h, z) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell\u001b[39m.\u001b[39;49mforward(embeds[i], actions[i], reset_masks[i], (h, z))\n\u001b[1;32m     52\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m         post, (h, z) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell\u001b[39m.\u001b[39mforward_prior(actions[i], reset_masks[i], (h, z))  \u001b[39m# open loop: post=prior\u001b[39;00m\n",
      "File \u001b[0;32m~/work/pyDreamer/pydreamer-minigrid/pydreamer/models/rssm.py:146\u001b[0m, in \u001b[0;36mRSSMCell.forward\u001b[0;34m(self, embed, action, reset_mask, in_state)\u001b[0m\n\u001b[1;32m    144\u001b[0m post_in \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(x)\n\u001b[1;32m    145\u001b[0m post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_mlp(post_in)                                    \u001b[39m# (B, S*S)\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m post_distr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzdistr(post)\n\u001b[1;32m    147\u001b[0m sample \u001b[39m=\u001b[39m post_distr\u001b[39m.\u001b[39mrsample()\u001b[39m.\u001b[39mreshape(B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    150\u001b[0m     post,                         \u001b[39m# tensor(B, 2*S)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     (h, sample),                  \u001b[39m# tensor(B, D+S+G)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m )\n",
      "File \u001b[0;32m~/work/pyDreamer/pydreamer-minigrid/pydreamer/models/rssm.py:198\u001b[0m, in \u001b[0;36mRSSMCell.zdistr\u001b[0;34m(self, pp)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstoch_discrete:\n\u001b[1;32m    197\u001b[0m     logits \u001b[39m=\u001b[39m pp\u001b[39m.\u001b[39mreshape(pp\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstoch_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstoch_discrete))\n\u001b[0;32m--> 198\u001b[0m     distr \u001b[39m=\u001b[39m D\u001b[39m.\u001b[39;49mOneHotCategoricalStraightThrough(logits\u001b[39m=\u001b[39;49mlogits\u001b[39m.\u001b[39;49mfloat())  \u001b[39m# NOTE: .float() needed to force float32 on AMP\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     distr \u001b[39m=\u001b[39m D\u001b[39m.\u001b[39mindependent\u001b[39m.\u001b[39mIndependent(distr, \u001b[39m1\u001b[39m)  \u001b[39m# This makes d.entropy() and d.kl() sum over stoch_dim\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m distr\n",
      "File \u001b[0;32m~/work/pyDreamer/env/lib/python3.8/site-packages/torch/distributions/one_hot_categorical.py:43\u001b[0m, in \u001b[0;36mOneHotCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, probs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, logits\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, validate_args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_categorical \u001b[39m=\u001b[39m Categorical(probs, logits)\n\u001b[1;32m     44\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_categorical\u001b[39m.\u001b[39mbatch_shape\n\u001b[1;32m     45\u001b[0m     event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_categorical\u001b[39m.\u001b[39mparam_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/work/pyDreamer/env/lib/python3.8/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m(Categorical, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/work/pyDreamer/env/lib/python3.8/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[1;32m     54\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[0;32m---> 55\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m     56\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m             )\n\u001b[1;32m     63\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Compare 2 images\n",
    "in_state_new = (in_state[0].to(device),in_state[1].to(device))\n",
    "\n",
    "with torch.no_grad():#imag_horizon=15,\n",
    "    losses, new_state, loss_metrics, tensors, dream_tensors = model.training_step(obs,in_state_new,imag_horizon=15,do_image_pred=True,do_dream_tensors=True)#Changed to model.training_step\n",
    "\n",
    "#pick image and dream\n",
    "origin_prediction = tensor_to_image(tensors,image_num=2,dream_num=2)\n",
    "del tensors\n",
    "\n",
    "#now we compare the difference when modyfing h\n",
    "avg1 = save_avg(obs,in_state_new,origin_prediction.shape,save_path=\"images/avg_new_unmodif\",image_num=2,dream_num=2,l=100)\n",
    "# copy = (in_state_new[0].clone(),in_state_new[1].clone())\n",
    "#modify the in state and see the effect\n",
    "a = torch.zeros_like(in_state_new[1])#set everything to zero in h\n",
    "b = in_state_new[0].clone()\n",
    "in_state_new = (b,a)\n",
    "\n",
    "avg2 = save_avg(obs,in_state_new,origin_prediction.shape,save_path=\"images/avg_new_modif\",image_num=2,dream_num=2,l=100)\n",
    "\n",
    "diff = np.sum(np.abs(avg1 - avg2),axis=2)\n",
    "diff = diff.astype(np.uint8)\n",
    "im = Image.fromarray(diff)\n",
    "im.save(\"images/diff-dif_state.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare the difference when modifying the latent variables\n",
    "# in_state_new = (in_state[0].to(device),in_state[1].to(device))\n",
    "\n",
    "# with torch.no_grad():#imag_horizon=15,\n",
    "#     losses, new_state, loss_metrics, tensors, dream_tensors = model.training_step(obs,in_state_new,imag_horizon=15,do_image_pred=True,do_dream_tensors=True)#Changed to model.training_step\n",
    "\n",
    "# #pick image and dream\n",
    "# origin_prediction = tensor_to_image(tensors,image_num=2,dream_num=2)\n",
    "# del tensors\n",
    "\n",
    "# #now we compare the difference when modyfing h\n",
    "# avg1 = save_avg(obs,in_state_new,origin_prediction.shape,save_path=\"images/avg_new_unmodif\",image_num=2,dream_num=2,l=100)\n",
    "# copy = (in_state_new[0].clone(),in_state_new[1].clone())\n",
    "# #modify the in state and see the effect\n",
    "# a = torch.zeros_like(in_state_new[1])#set everything to zero in h\n",
    "# b = in_state_new[0].clone()\n",
    "# in_state_new = (b,a)\n",
    "\n",
    "# avg2 = save_avg(obs,in_state_new,origin_prediction.shape,save_path=\"images/avg_new_modif\",image_num=2,dream_num=2,l=100)\n",
    "\n",
    "# diff = np.sum(np.abs(avg1 - avg2),axis=2)\n",
    "# diff = diff.astype(np.uint8)\n",
    "# im = Image.fromarray(diff)\n",
    "# im.save(\"images/diff-dif_state.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the average of the output of the dreamer\n",
    "\n",
    "#drop each latent variable to zero and see the image prediction\n",
    "# torch.manual_seed(0)\n",
    "# torch.use_deterministic_algorithms(mode=True,warn_only=True) \n",
    "# with torch.no_grad():#imag_horizon=15,\n",
    "#     losses, new_state, loss_metrics, tensors, dream_tensors = model.training_step(obs,in_state_new,imag_horizon=15,do_image_pred=True,do_dream_tensors=True)#Changed to model.training_step\n",
    "\n",
    "# #pick image and dream\n",
    "# copy = (in_state_new[0].clone(),in_state_new[1].clone())\n",
    "# origin_prediction = tensor_to_image(tensors,image_num=2,dream_num=2)\n",
    "# del tensors\n",
    "\n",
    "# print(origin_prediction.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tse interpretation\n",
    "# I need to generate many states for this to make sens\n",
    "# pca_50 = PCA(n_components=50)\n",
    "# pca_result_50 = pca_50.fit_transform(data_subset)\n",
    "# tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "# tsne_pca_results = tsne.fit_transform(pca_result_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolation\n",
    "# interpolate between two states, let's just get a list of state [ ]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b689f4da05e5af6be22d536205ccd9d8c0d85737225c21244d36d7e034a8a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
